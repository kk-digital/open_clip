{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## This for just testing the classification between two classes from Caltech100"
      ],
      "metadata": {
        "id": "YzDaTSk4zZaE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DfeXU4J1ylSo"
      },
      "outputs": [],
      "source": [
        "#@title Download Dataset\n",
        "%%capture\n",
        "!wget https://data.caltech.edu/records/nyy15-4j048/files/256_ObjectCategories.tar\n",
        "!tar -xvf /content/256_ObjectCategories.tar"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Install OpenCLIP\n",
        "%%capture\n",
        "!pip install open_clip_torch"
      ],
      "metadata": {
        "id": "eWkSHNfRzX43"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Imports\n",
        "import torch\n",
        "import torchvision\n",
        "import os\n",
        "import open_clip\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "j1KmWlxKzk4y"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Delete Unwanted Folders and Make Only Two Classes\n",
        "directory = '/content/256_ObjectCategories' # Directory holds all the image's folders\n",
        "dir_list  =     ['029.cannon' , '026.cake' , '027.calculator' ,\n",
        "                '025.cactus','028.camel' , '024.butterfly'] # List of all the wnated folders \n",
        "# Remove other folders\n",
        "for folder in os.listdir(directory):\n",
        "    f = os.path.join(directory, folder)    \n",
        "    if folder not in dir_list:\n",
        "      !rm -r $f\n",
        "      continue\n"
      ],
      "metadata": {
        "id": "7vLIqtkhznMC"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Clip Model\n",
        "model, _, preprocess = open_clip.create_model_and_transforms('ViT-B-32',pretrained='openai')\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xFb5DWxnz0aG",
        "outputId": "886e8279-fa18-4bc8-c1bc-427be0d99d5c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|███████████████████████████████████████| 354M/354M [00:05<00:00, 64.5MiB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title get features function from OpenAI CLIP Github\n",
        "# https://github.com/openai/CLIP#linear-probe-evaluation\n",
        "def get_features(dataset):\n",
        "    all_features = []\n",
        "    all_labels = []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for images, labels in DataLoader(dataset, batch_size=100):\n",
        "            features = model.encode_image(images.to(device))\n",
        "\n",
        "            all_features.append(features)\n",
        "            all_labels.append(labels)\n",
        "\n",
        "    return torch.cat(all_features).cpu().numpy(), torch.cat(all_labels).cpu().numpy()"
      ],
      "metadata": {
        "id": "WEDfQidiz2yp"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Making Dataset Out Of Images' folder\n",
        "import random\n",
        "transform = preprocess\n",
        "dataset = torchvision.datasets.ImageFolder(directory, transform)\n",
        "n = len(dataset)  # total number of examples\n",
        "n_test = int(0.1 * n)  # take ~10% for test\n",
        "\n",
        "test_list = [] \n",
        "while len(test_list) < n_test:\n",
        "  rand = random.randint(0, n)\n",
        "  if rand not in test_list:\n",
        "    test_list.append(rand)\n",
        "\n",
        "train_list = []\n",
        "for num in range(n):\n",
        "  if num not in test_list:\n",
        "    train_list.append(num)\n",
        "\n",
        "test_set = torch.utils.data.Subset(dataset, test_list,)  # take 10%\n",
        "train_set = torch.utils.data.Subset(dataset, train_list)"
      ],
      "metadata": {
        "id": "KCzcJIWYz6Vc"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.classes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UycV50yq0D47",
        "outputId": "614d8a31-5441-4832-9816-f991a231b6eb"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['024.butterfly',\n",
              " '025.cactus',\n",
              " '026.cake',\n",
              " '027.calculator',\n",
              " '028.camel',\n",
              " '029.cannon']"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title  Calculating images' features for train/test sets.\n",
        "train_features, train_labels = get_features(train_set)"
      ],
      "metadata": {
        "id": "XsldxbhN0F28"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_features, test_labels = get_features(test_set)"
      ],
      "metadata": {
        "id": "5bKpfVNUbol2"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_labels"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FbUqXAuL0JHF",
        "outputId": "43815ade-ab9d-4bd1-97e7-75325218bda8"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 3, 1, 2, 5, 4, 3, 0, 1, 1, 0, 1, 3, 5, 5, 1, 1, 2, 1, 5, 1, 0,\n",
              "       3, 0, 2, 5, 4, 1, 5, 3, 2, 5, 5, 3, 2, 3, 1, 3, 3, 1, 2, 4, 4, 4,\n",
              "       0, 3, 5, 5, 1, 2, 2, 1, 2, 0, 2, 1, 0, 3, 2, 0, 1, 4, 1, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using LogisticRegression"
      ],
      "metadata": {
        "id": "mRhkiz_s9BWw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the image features\n",
        "train_features, train_labels = get_features(train_set)\n",
        "test_features, test_labels = get_features(test_set)\n",
        "\n",
        "# Perform logistic regression\n",
        "## max_iter reduced to 100 \n",
        "classifier = LogisticRegression(random_state=0, max_iter=1000, verbose=1)\n",
        "classifier.fit(train_features, train_labels)\n",
        "\n",
        "# Evaluate using the logistic regression classifier\n",
        "predictions = classifier.predict(test_features)\n",
        "accuracy = np.mean((test_labels == predictions).astype(np.float)) * 100.\n",
        "print(f\"Accuracy = {accuracy:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NRSqSSbd9G_N",
        "outputId": "2b26f01d-2284-4e63-ee08-5590483dd48b"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy = 100.000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  \n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.3s finished\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:12: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  if sys.path[0] == '':\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predictions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PMjQE4B3_nT0",
        "outputId": "27988564-1279-4891-bc5b-fc64fae993c6"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 3, 1, 2, 5, 4, 3, 0, 1, 1, 0, 1, 3, 5, 5, 1, 1, 2, 1, 5, 1, 0,\n",
              "       3, 0, 2, 5, 4, 1, 5, 3, 2, 5, 5, 3, 2, 3, 1, 3, 3, 1, 2, 4, 4, 4,\n",
              "       0, 3, 5, 5, 1, 2, 2, 1, 2, 0, 2, 1, 0, 3, 2, 0, 1, 4, 1, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_labels"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OxRjnOuHAU8f",
        "outputId": "c8266daa-f573-4355-b725-0a8aa6959ec1"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 3, 1, 2, 5, 4, 3, 0, 1, 1, 0, 1, 3, 5, 5, 1, 1, 2, 1, 5, 1, 0,\n",
              "       3, 0, 2, 5, 4, 1, 5, 3, 2, 5, 5, 3, 2, 3, 1, 3, 3, 1, 2, 4, 4, 4,\n",
              "       0, 3, 5, 5, 1, 2, 2, 1, 2, 0, 2, 1, 0, 3, 2, 0, 1, 4, 1, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Test LogisticRegression using internet images\n",
        "from PIL import Image\n",
        "images_dir = '/content/drive/MyDrive/Brandon/butterflies'\n",
        "print(\"Classsifying the images in the butterfly directoy\")\n",
        "for file_name in os.listdir(images_dir):\n",
        "  image_path = os.path.join(images_dir , file_name)\n",
        "  image = preprocess(Image.open(image_path)).unsqueeze(0)\n",
        "  image_features =  model.encode_image(image)\n",
        "  print(dataset.classes[classifier.predict(image_features.detach().numpy()).item()])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f8QIAb0KAW4q",
        "outputId": "95327b0f-82d7-4fc2-db9c-735f7ed6499d"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classsifying the images in the butterfly directoy\n",
            "024.butterfly\n",
            "024.butterfly\n",
            "024.butterfly\n",
            "024.butterfly\n",
            "024.butterfly\n",
            "024.butterfly\n",
            "024.butterfly\n",
            "024.butterfly\n",
            "024.butterfly\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "images_dir = '/content/drive/MyDrive/Brandon/calculators'\n",
        "print(\"Classsifying the images in the claculators directoy\")\n",
        "for file_name in os.listdir(images_dir):\n",
        "  image_path = os.path.join(images_dir , file_name)\n",
        "  image = preprocess(Image.open(image_path)).unsqueeze(0)\n",
        "  image_features =  model.encode_image(image)\n",
        "  print(dataset.classes[classifier.predict(image_features.detach().numpy()).item()])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tkNdV3K1Ezse",
        "outputId": "e3d63fb9-5edf-440b-de5b-86ac7f6e4c26"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classsifying the images in the claculators directoy\n",
            "027.calculator\n",
            "027.calculator\n",
            "027.calculator\n",
            "027.calculator\n",
            "027.calculator\n",
            "027.calculator\n",
            "027.calculator\n",
            "027.calculator\n",
            "027.calculator\n",
            "027.calculator\n",
            "027.calculator\n",
            "027.calculator\n"
          ]
        }
      ]
    }
  ]
}