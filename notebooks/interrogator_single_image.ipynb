{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Interrogate Notebook\n",
        "> Make sure you put the .txt file in wordlists folder \n"
      ],
      "metadata": {
        "id": "totvcISyQowm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Imports\n",
        "try : \n",
        "  import open_clip\n",
        "except ImportError:\n",
        "  !pip install open_clip_torch\n",
        "  import open_clip\n",
        "\n",
        "try:\n",
        "  import torch\n",
        "except ImportError:\n",
        "  !pip install torch>=1.9.0\n",
        "  import torch\n",
        "\n",
        "try:\n",
        "  import PIL\n",
        "  from PIL import Image\n",
        "except:\n",
        "  !pip install pillow\n",
        "  import PIL\n",
        "  from PIL import Image\n",
        "\n",
        "import pathlib"
      ],
      "metadata": {
        "id": "7JSoWTxi6I1X"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "-xXR9dNYOA93"
      },
      "outputs": [],
      "source": [
        "#@title Load wordlist file function\n",
        "def load_list(filename):\n",
        "    with open(filename, 'r', encoding='utf-8', errors='replace') as f:\n",
        "        items = [line.strip() for line in f.readlines()]\n",
        "    return items"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Interrogate Single Image Function\n",
        "def interrogate_single_image(model  , \n",
        "                             preprocess,\n",
        "                             image_file = None, \n",
        "                             text_file = None ,\n",
        "                           ):\n",
        "  cwd_path = pathlib.Path.cwd().parent\n",
        "  if image_file is None :\n",
        "\n",
        "    image_file = cwd_path / 'images' / 'download_blue.png'\n",
        "  \n",
        "  if text_file is None :\n",
        "    text_file = cwd_path / 'wordlists' / 'wordlist.txt'\n",
        "\n",
        "  image = preprocess(Image.open(image_file)).unsqueeze(0)\n",
        "  txt_list = load_list(text_file)\n",
        "  text = open_clip.tokenize(txt_list)\n",
        "  \n",
        "  with torch.no_grad():\n",
        "      image_features = model.encode_image(image)\n",
        "      text_features = model.encode_text(text)\n",
        "      image_features /= image_features.norm(dim=-1, keepdim=True)\n",
        "      text_features /= text_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "      text_probs = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
        "      index = text_probs.argmax()\n",
        "      return txt_list[index]"
      ],
      "metadata": {
        "id": "Ljp78LeUPdr_"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Choose one of these models :: model name and pretrained \n",
        "open_clip.list_pretrained()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_dWOGDCxR9DI",
        "outputId": "3285652d-e1cf-43e7-ec6f-c815ea6d7dc9"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('RN50', 'openai'),\n",
              " ('RN50', 'yfcc15m'),\n",
              " ('RN50', 'cc12m'),\n",
              " ('RN50-quickgelu', 'openai'),\n",
              " ('RN50-quickgelu', 'yfcc15m'),\n",
              " ('RN50-quickgelu', 'cc12m'),\n",
              " ('RN101', 'openai'),\n",
              " ('RN101', 'yfcc15m'),\n",
              " ('RN101-quickgelu', 'openai'),\n",
              " ('RN101-quickgelu', 'yfcc15m'),\n",
              " ('RN50x4', 'openai'),\n",
              " ('RN50x16', 'openai'),\n",
              " ('RN50x64', 'openai'),\n",
              " ('ViT-B-32', 'openai'),\n",
              " ('ViT-B-32', 'laion400m_e31'),\n",
              " ('ViT-B-32', 'laion400m_e32'),\n",
              " ('ViT-B-32', 'laion2b_e16'),\n",
              " ('ViT-B-32', 'laion2b_s34b_b79k'),\n",
              " ('ViT-B-32-quickgelu', 'openai'),\n",
              " ('ViT-B-32-quickgelu', 'laion400m_e31'),\n",
              " ('ViT-B-32-quickgelu', 'laion400m_e32'),\n",
              " ('ViT-B-16', 'openai'),\n",
              " ('ViT-B-16', 'laion400m_e31'),\n",
              " ('ViT-B-16', 'laion400m_e32'),\n",
              " ('ViT-B-16-plus-240', 'laion400m_e31'),\n",
              " ('ViT-B-16-plus-240', 'laion400m_e32'),\n",
              " ('ViT-L-14', 'openai'),\n",
              " ('ViT-L-14', 'laion400m_e31'),\n",
              " ('ViT-L-14', 'laion400m_e32'),\n",
              " ('ViT-L-14', 'laion2b_s32b_b82k'),\n",
              " ('ViT-L-14-336', 'openai'),\n",
              " ('ViT-H-14', 'laion2b_s32b_b79k'),\n",
              " ('ViT-g-14', 'laion2b_s12b_b42k')]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Make sure you choosed the desired model\n",
        "model, _, preprocess = open_clip.create_model_and_transforms('ViT-B-32',pretrained='openai')"
      ],
      "metadata": {
        "id": "8tDhy38HRNX9"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# run the interrogate_single_image function \n",
        "# returns ==> the most matching text in the input text\n",
        "interrogate_single_image(model = model , preprocess=preprocess)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "CR1qX70iSv2-",
        "outputId": "0bda18bc-6477-45da-ea1f-ef59f04eaf76"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'pixel art'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    }
  ]
}